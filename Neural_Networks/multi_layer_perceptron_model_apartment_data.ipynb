{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f40d2887",
   "metadata": {},
   "source": [
    "# Multi-Layer Perceptron (MLP) Model (apartment data)\n",
    "\n",
    "In this notebook, we will use a Multi-Layer Perceptron (MLP) Model to predict rental prices of apartments. A Multi-Layer Perceptron (MLP) model is a type of artificial neural network that consists of multiple layers of neurons. Each neuron in a layer is connected to every neuron in the previous and next layers, forming a fully connected network. The MLP model is capable of learning complex patterns in the data through backpropagation and gradient descent.\n",
    "\n",
    "The key components of an MLP model include:\n",
    "- **Input Layer**: The layer that receives the input features.\n",
    "- **Hidden Layers**: One or more layers with weights and activation functions to learn intermediate representations.\n",
    "- **Output Layer**: The layer that produces the final prediction.\n",
    "\n",
    "In this notebook, we will:\n",
    "1. Preprocess the apartment data, including scaling the features.\n",
    "2. Define and compile the MLP model using TensorFlow/Keras.\n",
    "3. Train the model on the training data.\n",
    "4. Evaluate the model on the test data.\n",
    "5. Visualize the training process and the model's performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b71a018",
   "metadata": {},
   "source": [
    "## Libraries and settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66dc5534",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import tree\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Show current working directory\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c0ced7",
   "metadata": {},
   "source": [
    "## Import the apartment data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0abc23fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define columns for import\n",
    "columns = [ 'web-scraper-order',\n",
    "            'address_raw',\n",
    "            'rooms',\n",
    "            'area',\n",
    "            'luxurious',\n",
    "            'price',\n",
    "            'price_per_m2',\n",
    "            'lat',\n",
    "            'lon',\n",
    "            'bfs_number',\n",
    "            'bfs_name',\n",
    "            'pop',\n",
    "            'pop_dens',\n",
    "            'frg_pct',\n",
    "            'emp',\n",
    "            'mean_taxable_income',\n",
    "            'dist_supermarket']\n",
    "\n",
    "# Read and select variables\n",
    "df_orig = pd.read_csv(\"./Data/apartments_data_enriched_cleaned.csv\", sep=\";\", encoding='utf-8')[columns]\n",
    "\n",
    "# Rename variable 'web-scraper-order' to 'apmt_id'\n",
    "df_orig = df_orig.rename(columns={'web-scraper-order': 'id'})\n",
    "\n",
    "# Remove missing values\n",
    "df = df_orig.dropna()\n",
    "df.head(5)\n",
    "\n",
    "# Remove duplicates\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "# Remove some 'extreme' values\n",
    "df = df.loc[(df['price'] >= 1000) & \n",
    "            (df['price'] <= 5000)]\n",
    "\n",
    "# Reset index\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "print(df.shape)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82198a19",
   "metadata": {},
   "source": [
    "## Rescale features using a Min-Max Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d76a5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "### automatischer MinMax-scaler, viel einfacher als bei der lineraen regression\n",
    "### 'scaling' ist vorteilhaft für neuronale Netze, da die Gewichte sonst zu stark schwanken\n",
    "\n",
    "# List of features to re-scale\n",
    "features_to_scale = ['area', \n",
    "                     'rooms',\n",
    "                     'lat',\n",
    "                     'lon',\n",
    "                     'pop',\n",
    "                     'pop_dens',\n",
    "                     'frg_pct',\n",
    "                     'emp',\n",
    "                     'mean_taxable_income',\n",
    "                     'dist_supermarket']\n",
    "\n",
    "# Initialize the MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Fit and transform the features\n",
    "df[features_to_scale] = scaler.fit_transform(df[features_to_scale])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0b020e",
   "metadata": {},
   "source": [
    "## Create train and test samples (train = 80%, test = 20% of the data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28d6db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train and test samples\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[features_to_scale], \n",
    "                                                    df['price'], \n",
    "                                                    test_size=0.20, \n",
    "                                                    random_state=42)\n",
    "\n",
    "# Show X_train\n",
    "print('X_train:')\n",
    "print(X_train.head(), '\\n')\n",
    "\n",
    "# Show y_train\n",
    "print('y_train:')\n",
    "print(y_train.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f65098",
   "metadata": {},
   "source": [
    "## Train the Multi-Layer Perceptron (MLP) Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336b3213",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of features\n",
    "num_features = X_train.shape[1]\n",
    "\n",
    "# Define the model with dropout\n",
    "model = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(num_features,)),  # Hidden layer 1\n",
    "    Dropout(0.2),                                               # Dropout layer 1\n",
    "    Dense(32, activation='relu'),                               # Hidden layer 2\n",
    "    Dropout(0.2),                                               # Dropout layer 2\n",
    "    Dense(16, activation='relu'),                               # Hidden layer 3\n",
    "    Dropout(0.2),                                               # Dropout layer 3\n",
    "    Dense(1)                                                    # Output layer\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, \n",
    "                    y_train, \n",
    "                    epochs=100, \n",
    "                    validation_split=0.20, \n",
    "                    batch_size=32,\n",
    "                    verbose=1)\n",
    "\n",
    "# Evaluate the model on the test set using the mean absolute error (MAE)\n",
    "test_loss, test_mae = model.evaluate(X_test, y_test)\n",
    "# print(f\"Test MAE: {test_mae}\")\n",
    "\n",
    "# Predict the response for test dataset\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate R2 score\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f\"R2 score: {r2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ea38ed",
   "metadata": {},
   "source": [
    "## Plot training & validation loss values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e20210",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation MAE values\n",
    "plt.plot(history.history['mae'])\n",
    "plt.plot(history.history['val_mae'])\n",
    "plt.title('Model MAE')\n",
    "plt.ylabel('MAE')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5dc21b7",
   "metadata": {},
   "source": [
    "### Jupyter notebook --footer info-- (please always provide this at the end of each notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6908ed12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import platform\n",
    "import socket\n",
    "from platform import python_version\n",
    "from datetime import datetime\n",
    "\n",
    "print('-----------------------------------')\n",
    "print(os.name.upper())\n",
    "print(platform.system(), '|', platform.release())\n",
    "print('Datetime:', datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "print('Python Version:', python_version())\n",
    "print('-----------------------------------')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
